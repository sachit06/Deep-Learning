# -*- coding: utf-8 -*-
"""Sachit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13XQFSzFNmMN0xcNzzzWDlEso2jCLhzEw
"""

#Importing essential libraries.

from google.colab import drive 
import os
import tensorflow as tf
from numpy import asarray
from numpy import save
from keras.models import Sequential
from keras.layers import Convolution2D
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten, BatchNormalization,Activation,Dropout
from keras.layers import Dense
from tensorflow.keras import regularizers
from sklearn import metrics
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from keras.losses import categorical_crossentropy
from sklearn.metrics import classification_report
import sklearn.metrics as metrics
import numpy as np
import keras
from keras.callbacks import *
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import classification_report,confusion_matrix
from tensorflow.keras.utils import Sequence
import zipfile
from math import pi
from math import cos
from math import floor
import sys
from keras import backend

# Command to mount your google drive
from google.colab import drive
drive.mount('/content/drive')

"""
The images have been read from compressed folders of train and valid.
You can use compressed train and valid folders if you want to run the network.
The path can be specified below
"""

zip_ref1 = zipfile.ZipFile("/content/drive/MyDrive/train.zip", 'r')
zip_ref1.extractall("/datalab")
zip_ref1.close()
zip_ref1 = zipfile.ZipFile("/content/drive/MyDrive/valid.zip", 'r')
zip_ref1.extractall("/datalab")
zip_ref1.close()

from tensorflow.keras.preprocessing.image import ImageDataGenerator
"""
ImageDataGenerator is used to do different manipulations on images.
Image augmentations have been done by shifting height,shear_range,zoom,width shift,vertical flip
Image has been scaled for faster execution.
Target image size has been taken as 128x128
Batchsize of 32 seems to be working better than 16 and 64
Classmode is categorical because there are 11 classes
"""
train_datagen = ImageDataGenerator(
    height_shift_range = 0.4,
    validation_split = 0.2,
    horizontal_flip=True,
    rescale=1./255,
    shear_range=0.4,
    zoom_range=0.2,
    width_shift_range = 0.3,
    vertical_flip = 0.2)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    '/datalab/train',
    target_size=( 128, 128),
    batch_size=32,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    '/datalab/valid',
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical')
#Implementation of cycliclearning rate class
class cycliclearningrate(Callback):
        
        name = []

        def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):
            self.epochs = n_epochs
            self.cycles = n_cycles
            self.lr_max = lrate_max
            self.lrates = list()
        
        def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):
            epochs_per_cycle = floor(n_epochs/n_cycles)
            cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)
            return lrate_max/2 * (cos(cos_inner) + 1)
        

        def on_epoch_begin(self, epoch, logs={}):
            lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)

            backend.set_value(self.model.optimizer.lr, lr)

            self.lrates.append(lr)
"""
Checkpoint has been set to save the model with best validation accuracy
Added BatchNormalization,Dropout,MaxPooling2D, L2 kernel_regularizer for penalizing weights
Used relu activation and softmax to predict the classes.
Adam optimizer has been used for final model as it is performing better than other optimizers
A lower learning rate has been used to prevent overshooting and learning_rate has been decayed with epochs
Validation accuracy is plotted to visualize it
75 epochs have been used as the validation accuracy is not improving further after 75 epochs.
"""
weight_decay = 1e-4
checkpoint = ModelCheckpoint('/content/drive/MyDrive/saved_model/weights.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

model = Sequential()

model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay),input_shape = (128,128,3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.1))

model.add(Flatten())
model.add(Dense(11))
model.add(Activation('softmax'))
cycles = 9
callback = cycliclearningrate(90, cycles, 0.0005)
 
opt = keras.optimizers.Adam(learning_rate=0.0005)
model.compile(optimizer = opt, loss = categorical_crossentropy, metrics = ['accuracy'])
history = model.fit(train_generator,validation_data=test_generator, epochs=90, verbose=1,callbacks=[checkpoint,callback])
plt.plot(history.history['val_accuracy'])

